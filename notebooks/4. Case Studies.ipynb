{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Studies\n",
    "\n",
    "Recall from the introduction that a common statistical analysis is to compare two groups. In a frequentist setting, statistical hypothesis testing is often applied as a default approach. In a Bayesian setting, we will instead use **estimation** to tackle this problem, in the hopes of obtaining more reliable inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation for one group\n",
    "\n",
    "Before we compare two groups using Bayesian analysis, let's start with an even simpler scenario: statistical inference for one group.\n",
    "\n",
    "For this we will use Gelman et al.'s (2007) radon dataset. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\n",
    "\n",
    ">  the US EPA has set an action level of 4 pCi/L. At or above this level of radon, the EPA recommends you take corrective measures to reduce your exposure to radon gas.\n",
    "\n",
    "![radon](images/radon_entry.jpg)\n",
    "\n",
    "Let's import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radon = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "radon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on the (log) radon levels measured in a single county (Hennepin). \n",
    "\n",
    "Suppose we are interested in:\n",
    "\n",
    "- whether the mean log-radon value is greater than 4 pCi/L in Hennepin county\n",
    "- the probability that any randomly-chosen household in Hennepin county has a reading of greater than 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hennepin_radon = radon.query('county==\"HENNEPIN\"').log_radon\n",
    "sns.distplot(hennepin_radon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hennepin_radon.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Recall that the first step in Bayesian inference is specifying a **full probability model** for the problem.\n",
    "\n",
    "This consists of:\n",
    "\n",
    "- a likelihood function(s) for the observations\n",
    "- priors for all unknown quantities\n",
    "\n",
    "The measurements look approximately normal, so let's start by assuming a normal distribution as the sampling distribution (likelihood) for the data. \n",
    "\n",
    "$$y_i \\sim N(\\mu, \\sigma^2)$$\n",
    "\n",
    "(don't worry, we can evaluate this assumption)\n",
    "\n",
    "This implies that we have 2 unknowns in the model; the mean and standard deviation of the distribution. \n",
    "\n",
    "#### Prior choice\n",
    "\n",
    "How do we choose distributions to use as priors for these parameters? \n",
    "\n",
    "There are several considerations:\n",
    "\n",
    "- discrete vs continuous values\n",
    "- the support of the variable\n",
    "- the available prior information\n",
    "\n",
    "While there may likely be prior information about the distribution of radon values, we will assume no prior knowledge, and specify a **diffuse** prior for each parameter.\n",
    "\n",
    "Since the mean can take any real value (since it is on the log scale), we will use another normal distribution here, and specify a large variance to allow the possibility of very large or very small values:\n",
    "\n",
    "$$\\mu \\sim N(0, 10^2)$$\n",
    "\n",
    "For the standard deviation, we know that the true value must be positive (no negative variances!). I will choose a uniform prior bounded from below at zero and from above at a value that is sure to be higher than any plausible value the true standard deviation (on the log scale) could take.\n",
    "\n",
    "$$\\sigma \\sim U(0, 10)$$\n",
    "\n",
    "We can encode these in a Python model, using the PyMC3 package, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, Uniform\n",
    "\n",
    "with Model() as radon_model:\n",
    "    \n",
    "    μ = Normal('μ', mu=0, sd=10)\n",
    "    σ = Uniform('σ', 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Software\n",
    "> Today there is an array of software choices for Bayesians, including both open source software (*e.g.*, Stan, PyMC, JAGS, emcee) and commercial (*e.g.*, SAS, Stata). These examples can be replicated in any of these environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to add the likelihood, which takes $\\mu$ and $\\sigma$ as parameters, and the log-radon values as the set of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with radon_model:\n",
    "    \n",
    "    y = Normal('y', mu=μ, sd=σ, observed=hennepin_radon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will fit the model using a numerical approach called **variational inference**. This will estimate the posterior distribution using an optimized approximation, and then draw samples from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import fit\n",
    "\n",
    "with radon_model:\n",
    "\n",
    "    samples = fit(random_seed=RANDOM_SEED).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import plot_posterior\n",
    "\n",
    "plot_posterior(samples, varnames=['μ'], ref_val=np.log(4), color='LightSeaGreen');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the posterior distribution of $\\mu$, along with an estimate of the 95% posterior **credible interval**. \n",
    "\n",
    "The output\n",
    "\n",
    "    83.1% < 1.38629 < 16.9%%\n",
    "    \n",
    "informs us that the probability of $\\mu$ being less than $\\log(4)$ is 83.1%% and the corresponding probability of being greater than $\\log(4)$ is 16.9%.\n",
    "\n",
    "> The posterior probability that the mean level of household radon in Henneprin County is greater than 4 pCi/L is 0.17."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "\n",
    "What is the probability that a given household has a log-radon measurement larger than one? To answer this, we make use of the **posterior predictive distribution**.\n",
    "\n",
    "$$p(z |y) = \\int_{\\theta} p(z |\\theta) p(\\theta | y) d\\theta$$\n",
    "\n",
    "where here $z$ is the predicted value and y is the data used to fit the model.\n",
    "\n",
    "We can estimate this from the posterior samples of the parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mus = samples['μ']\n",
    "sigmas = samples['σ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "radon_samples = Normal.dist(mus, sigmas).random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(radon_samples > np.log(4)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The posterior probability that a randomly-selected household in Henneprin County contains radon levels in excess of 4 pCi/L is 0.48."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Groups with Continiuous Outcome\n",
    "\n",
    "To illustrate how this Bayesian estimation approach works in practice, we will use a fictitious example from Kruschke (2012) concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a \"smart drug\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment and control arms, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pd.DataFrame(dict(iq=(101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101),\n",
    "                         group='drug'))\n",
    "placebo = pd.DataFrame(dict(iq=(99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99),\n",
    "                            group='placebo'))\n",
    "\n",
    "trial_data = pd.concat([drug, placebo], ignore_index=True)\n",
    "trial_data.hist('iq', by='group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there appear to be extreme (\"outlier\") values in the data, we will choose a Student-t distribution to describe the distributions of the scores in each group. This sampling distribution adds **robustness** to the analysis, as a T distribution is less sensitive to outlier observations, relative to a normal distribution. \n",
    "\n",
    "The three-parameter Student-t distribution allows for the specification of a mean $\\mu$, a precision (inverse-variance) $\\lambda$ and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "the degrees-of-freedom parameter essentially specifies the \"normality\" of the data, since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails.\n",
    "\n",
    "Thus, the likelihood functions of our model are specified as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "y^{(drug)}_i &\\sim T(\\nu, \\mu_1, \\sigma_1) \\\\\n",
    "y^{(placebo)}_i &\\sim T(\\nu, \\mu_2, \\sigma_2)\n",
    "\\end{align}$$\n",
    "\n",
    "As a simplifying assumption, we will assume that the degree of normality $\\nu$ is the same for both groups. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Draw 10000 samples from a Student-T distribution (`StudentT` in PyMC3) with parameter `nu=3` and compare the distribution of these values to a similar number of draws from a Normal distribution with parameters `mu=0` and `sd=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import StudentT\n",
    "\n",
    "t = StudentT.dist(nu=3).random(size=10000)\n",
    "n = Normal.dist(0, 1).random(size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(t, label='Student-T')\n",
    "sns.distplot(n, label='Normal')\n",
    "plt.legend()\n",
    "plt.xlim(-10,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prior choice\n",
    "\n",
    "Since the means are real-valued, we will apply normal priors. Since we know something about the population distribution of IQ values, we will center the priors at 100, and use a standard deviation that is more than wide enough to account for plausible deviations from this population mean:\n",
    "\n",
    "$$\\mu_k \\sim N(100, 10^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Model() as drug_model:\n",
    "    \n",
    "    μ_0 = Normal('μ_0', 100, sd=10)\n",
    "    μ_1 = Normal('μ_1', 100, sd=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will use a uniform prior for the standard deviations, with an upper bound of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    σ_0 = Uniform('σ_0', lower=0, upper=20)\n",
    "    σ_1 = Uniform('σ_1', lower=0, upper=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the degrees-of-freedom parameter $\\nu$, we will use an **exponential** distribution with a mean of 30; this allocates high prior probability over the regions of the parameter that describe the range from normal to heavy-tailed data under the Student-T distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Exponential\n",
    "\n",
    "with drug_model:\n",
    "    ν = Exponential('ν_minus_one', 1/29.) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Exponential.dist(1/29).random(size=10000), kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import StudentT\n",
    "\n",
    "with drug_model:\n",
    "\n",
    "    drug_like = StudentT('drug_like', nu=ν, mu=μ_1, lam=σ_1**-2, observed=drug.iq)\n",
    "    placebo_like = StudentT('placebo_like', nu=ν, mu=μ_0, lam=σ_0**-2, observed=placebo.iq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is fully specified, we can turn our attention to tracking the posterior quantities of interest. Namely, we can calculate the difference in means between the drug and placebo groups.\n",
    "\n",
    "As a joint measure of the groups, we will also estimate the \"effect size\", which is the difference in means scaled by the pooled estimates of standard deviation. This quantity can be harder to interpret, since it is no longer in the same units as our data, but it is a function of all four estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "\n",
    "with drug_model:\n",
    "    \n",
    "    diff_of_means = Deterministic('difference of means', μ_1 - μ_0)\n",
    "    \n",
    "    effect_size = Deterministic('effect size', \n",
    "                        diff_of_means / np.sqrt((σ_1**2 + σ_0**2) / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    \n",
    "    drug_trace = fit(random_seed=RANDOM_SEED).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(drug_trace[100:], \n",
    "                varnames=['μ_0', 'μ_1', 'σ_0', 'σ_1', 'ν_minus_one'],\n",
    "                color='#87ceeb');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(drug_trace[100:], \n",
    "          varnames=['difference of means', 'effect size'],\n",
    "          ref_val=0,\n",
    "          color='#87ceeb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The posterior probability that the mean IQ of subjects in the treatment group is greater than that of the control group is 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Load the `nashville_precip.txt` dataset. Build a model to compare rainfall in January and July. \n",
    "\n",
    "- What's the probability that the expected rainfall in January is larger than in July?\n",
    "- What's the probability that January rainfall exceeds July rainfall in a given year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_precip = pd.read_table('../data/nashville_precip.txt', \n",
    "                            delimiter='\\s+', na_values='NA', index_col=0)\n",
    "nash_precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Groups with Binary Outcome\n",
    "\n",
    "Now that we have seen how to generalize normally-distributed data to another distribution, we are equipped to analyze other data types. Binary outcomes are common in clinical research: \n",
    "\n",
    "- survival/death\n",
    "- true/false\n",
    "- presence/absence\n",
    "- positive/negative\n",
    "\n",
    "> *Never, ever dichotomize continuous or ordinal variables prior to statistical analysis*\n",
    "\n",
    "In practice, binary outcomes are encoded as ones (for event occurrences) and zeros (for non-occurrence). A single binary variable is distributed as a **Bernoulli** random variable:\n",
    "\n",
    "$$f(x \\mid p) = p^{x} (1-p)^{1-x}$$\n",
    "\n",
    "Such events are sometimes reported as sums of individual events, such as the number of individuals in a group who test positive for a condition of interest. Sums of Bernoulli events are distributed as **binomial** random variables.\n",
    "\n",
    "$$f(x \\mid n, p) = \\binom{n}{x} p^x (1-p)^{n-x}$$\n",
    "\n",
    "The parameter in both models is $p$, the probability of the occurrence of an event. In terms of inference, we are typically interested in whether $p$ is larger or smaller in one group relative to another.\n",
    "\n",
    "To demonstrate the comparison of two groups with binary outcomes using Bayesian inference, we will use a sample pediatric dataset. Data on 671 infants with very low (<1600 grams) birth weight from 1981-87 were collected at Duke University Medical Center. Of interest is the relationship between the outcome intra-ventricular hemorrhage (IVH) and predictor such as birth weight, gestational age, presence of pneumothorax and mode of delivery.\n",
    "\n",
    "![](images/ivh.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlbw = pd.read_csv('../data/vlbw.csv', index_col=0).dropna(axis=0, subset=['ivh', 'pneumo'])\n",
    "vlbw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate binary data analysis, we will try to estimate the difference between the probability of an intra-ventricular hemorrhage for infants with a pneumothorax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(vlbw.ivh, vlbw.pneumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a binary outcome by combining `definite` and `possible` into a single outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ivh = vlbw.ivh.isin(['definite', 'possible']).astype(int).values\n",
    "x = vlbw.pneumo.astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior choice\n",
    "\n",
    "What should we choose as a prior distribution for $p$?\n",
    "\n",
    "We could stick with a normal distribution, but note that the value of $p$ is **constrained** by the laws of probability. Namely, we cannot have values smaller than zero nor larger than one. So, choosing a normal distribution will result in ascribing positive probability to unsupported values of the parameter. In many cases, this will still work in practice, but will be inefficient for calculating the posterior and will not accurately represent the prior information about the parameter.\n",
    "\n",
    "A common choice in this context is the **beta distribution**, a continuous distribution with 2 parameters and whose support is on the unit interval:\n",
    "\n",
    "$$ f(x \\mid \\alpha, \\beta) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "- Support: $x \\in (0, 1)$\n",
    "- Mean: $\\dfrac{\\alpha}{\\alpha + \\beta}$\n",
    "- Variance: $\\dfrac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Beta\n",
    "\n",
    "params = (5, 1), (1, 3), (5, 5), (0.5, 0.5), (1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(params), figsize=(14, 4), sharey=True)\n",
    "for ax, (alpha, beta) in zip(axes, params):\n",
    "    sns.distplot(Beta.dist(alpha, beta).random(size=10000), ax=ax, kde=False)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(r'$\\alpha={0}, \\beta={1}$'.format(alpha, beta));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's use a beta distribution to model our prior knowledge of the probabilities for both groups. Setting $\\alpha = \\beta = 1$ will result in a uniform distribution of prior mass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Model() as ivh_model:\n",
    "    \n",
    "    p = Beta('p', 1, 1, shape=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `p` as the parameter of our Bernoulli likelihood. Here, `x` is a vector of zeros an ones, which will extract the approproate group probability for each subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Bernoulli\n",
    "\n",
    "with ivh_model:\n",
    "    \n",
    "    bb_like = Bernoulli('bb_like', p=p[x], observed=ivh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we are interested in the difference between the probabilities, we will keep track of this difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with ivh_model:\n",
    "    \n",
    "    p_diff = Deterministic('p_diff', p[1] - p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ivh_model:\n",
    "    ivh_trace = fit(random_seed=RANDOM_SEED).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(ivh_trace[100:], varnames=['p'], color='#87ceeb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the probability that `p` is larger for the pneumothorax with probability one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(ivh_trace[100:], varnames=['p_diff'], ref_val=0, color='#87ceeb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "\n",
    "- Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis, Third Edition. CRC Press.\n",
    "- Kruschke, J.K. *Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan.* 2015. Academic Press / Elsevier. \n",
    "- O'Shea M, Savitz D.A., Hage M.L., Feinstein K.A.: *Prenatal events and the risk of subependymal / intraventricular haemorrhage in very low birth weight neonates*. **Paediatric and Perinatal Epdiemiology** 1992;6:352-362"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
