{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Groups with Continiuous Outcome\n",
    "\n",
    "To illustrate how this Bayesian estimation approach works in practice, we will use a fictitious example from Kruschke (2012) concerning the evaluation of a clinical trial for drug evaluation. The trial aims to evaluate the efficacy of a \"smart drug\" that is supposed to increase intelligence by comparing IQ scores of individuals in a treatment arm (those receiving the drug) to those in a control arm (those recieving a placebo). There are 47 individuals and 42 individuals in the treatment and control arms, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = pd.DataFrame(dict(iq=(101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n",
    "        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n",
    "        96,103,124,101,101,100,101,101,104,100,101),\n",
    "                         group='drug'))\n",
    "placebo = pd.DataFrame(dict(iq=(99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n",
    "           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n",
    "           101,100,99,101,100,102,99,100,99),\n",
    "                            group='placebo'))\n",
    "\n",
    "trial_data = pd.concat([drug, placebo], ignore_index=True)\n",
    "trial_data.hist('iq', by='group');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there appear to be extreme (\"outlier\") values in the data, we will choose a Student-t distribution to describe the distributions of the scores in each group. This sampling distribution adds **robustness** to the analysis, as a T distribution is less sensitive to outlier observations, relative to a normal distribution. \n",
    "\n",
    "The three-parameter Student-t distribution allows for the specification of a mean $\\mu$, a precision (inverse-variance) $\\lambda$ and a degrees-of-freedom parameter $\\nu$:\n",
    "\n",
    "$$f(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\Gamma(\\frac{\\nu}{2})} \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}} \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}$$\n",
    "           \n",
    "the degrees-of-freedom parameter essentially specifies the \"normality\" of the data, since larger values of $\\nu$ make the distribution converge to a normal distribution, while small values (close to zero) result in heavier tails.\n",
    "\n",
    "Thus, the likelihood functions of our model are specified as follows:\n",
    "\n",
    "$$\\begin{align}\n",
    "y^{(drug)}_i &\\sim T(\\nu, \\mu_1, \\sigma_1) \\\\\n",
    "y^{(placebo)}_i &\\sim T(\\nu, \\mu_2, \\sigma_2)\n",
    "\\end{align}$$\n",
    "\n",
    "As a simplifying assumption, we will assume that the degree of normality $\\nu$ is the same for both groups. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Draw 10000 samples from a Student-T distribution (`StudentT` in PyMC3) with parameter `nu=3` and compare the distribution of these values to a similar number of draws from a Normal distribution with parameters `mu=0` and `sd=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import StudentT\n",
    "\n",
    "t = StudentT.dist(nu=3).random(size=10000)\n",
    "n = Normal.dist(0, 1).random(size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(t, label='Student-T')\n",
    "sns.distplot(n, label='Normal')\n",
    "plt.legend()\n",
    "plt.xlim(-10,10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Prior choice\n",
    "\n",
    "Since the means are real-valued, we will apply normal priors. Since we know something about the population distribution of IQ values, we will center the priors at 100, and use a standard deviation that is more than wide enough to account for plausible deviations from this population mean:\n",
    "\n",
    "$$\\mu_k \\sim N(100, 10^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Model() as drug_model:\n",
    "    \n",
    "    μ_0 = Normal('μ_0', 100, sd=10)\n",
    "    μ_1 = Normal('μ_1', 100, sd=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will use a uniform prior for the standard deviations, with an upper bound of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    σ_0 = Uniform('σ_0', lower=0, upper=20)\n",
    "    σ_1 = Uniform('σ_1', lower=0, upper=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the degrees-of-freedom parameter $\\nu$, we will use an **exponential** distribution with a mean of 30; this allocates high prior probability over the regions of the parameter that describe the range from normal to heavy-tailed data under the Student-T distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Exponential\n",
    "\n",
    "with drug_model:\n",
    "    ν = Exponential('ν_minus_one', 1/29.) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(Exponential.dist(1/29).random(size=10000), kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import StudentT\n",
    "\n",
    "with drug_model:\n",
    "\n",
    "    drug_like = StudentT('drug_like', nu=ν, mu=μ_1, lam=σ_1**-2, observed=drug.iq)\n",
    "    placebo_like = StudentT('placebo_like', nu=ν, mu=μ_0, lam=σ_0**-2, observed=placebo.iq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is fully specified, we can turn our attention to tracking the posterior quantities of interest. Namely, we can calculate the difference in means between the drug and placebo groups.\n",
    "\n",
    "As a joint measure of the groups, we will also estimate the \"effect size\", which is the difference in means scaled by the pooled estimates of standard deviation. This quantity can be harder to interpret, since it is no longer in the same units as our data, but it is a function of all four estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Deterministic\n",
    "\n",
    "with drug_model:\n",
    "    \n",
    "    diff_of_means = Deterministic('difference of means', μ_1 - μ_0)\n",
    "    \n",
    "    effect_size = Deterministic('effect size', \n",
    "                        diff_of_means / np.sqrt((σ_1**2 + σ_0**2) / 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with drug_model:\n",
    "    \n",
    "    drug_trace = fit(random_seed=RANDOM_SEED).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(drug_trace[100:], \n",
    "                varnames=['μ_0', 'μ_1', 'σ_0', 'σ_1', 'ν_minus_one'],\n",
    "                color='#87ceeb');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(drug_trace[100:], \n",
    "          varnames=['difference of means', 'effect size'],\n",
    "          ref_val=0,\n",
    "          color='#87ceeb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The posterior probability that the mean IQ of subjects in the treatment group is greater than that of the control group is 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Load the `nashville_precip.txt` dataset. Build a model to compare rainfall in January and July. \n",
    "\n",
    "- What's the probability that the expected rainfall in January is larger than in July?\n",
    "- What's the probability that January rainfall exceeds July rainfall in a given year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nash_precip = pd.read_table('../data/nashville_precip.txt', \n",
    "                            delimiter='\\s+', na_values='NA', index_col=0)\n",
    "nash_precip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../exercises/rainfall.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Groups with Binary Outcome\n",
    "\n",
    "Now that we have seen how to generalize normally-distributed data to another distribution, we are equipped to analyze other data types. Binary outcomes are common in clinical research: \n",
    "\n",
    "- survival/death\n",
    "- true/false\n",
    "- presence/absence\n",
    "- positive/negative\n",
    "\n",
    "> *Never, ever dichotomize continuous or ordinal variables prior to statistical analysis*\n",
    "\n",
    "In practice, binary outcomes are encoded as ones (for event occurrences) and zeros (for non-occurrence). A single binary variable is distributed as a **Bernoulli** random variable:\n",
    "\n",
    "$$f(x \\mid p) = p^{x} (1-p)^{1-x}$$\n",
    "\n",
    "Such events are sometimes reported as sums of individual events, such as the number of individuals in a group who test positive for a condition of interest. Sums of Bernoulli events are distributed as **binomial** random variables.\n",
    "\n",
    "$$f(x \\mid n, p) = \\binom{n}{x} p^x (1-p)^{n-x}$$\n",
    "\n",
    "The parameter in both models is $p$, the probability of the occurrence of an event. In terms of inference, we are typically interested in whether $p$ is larger or smaller in one group relative to another.\n",
    "\n",
    "To demonstrate the comparison of two groups with binary outcomes using Bayesian inference, we will use a sample pediatric dataset. Data on 671 infants with very low (<1600 grams) birth weight from 1981-87 were collected at Duke University Medical Center. Of interest is the relationship between the outcome intra-ventricular hemorrhage (IVH) and predictor such as birth weight, gestational age, presence of pneumothorax and mode of delivery.\n",
    "\n",
    "![](images/ivh.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlbw = pd.read_csv('../data/vlbw.csv', index_col=0).dropna(axis=0, subset=['ivh', 'pneumo'])\n",
    "vlbw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate binary data analysis, we will try to estimate the difference between the probability of an intra-ventricular hemorrhage for infants with a pneumothorax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(vlbw.ivh, vlbw.pneumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a binary outcome by combining `definite` and `possible` into a single outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ivh = vlbw.ivh.isin(['definite', 'possible']).astype(int).values\n",
    "x = vlbw.pneumo.astype(int).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior choice\n",
    "\n",
    "What should we choose as a prior distribution for $p$?\n",
    "\n",
    "We could stick with a normal distribution, but note that the value of $p$ is **constrained** by the laws of probability. Namely, we cannot have values smaller than zero nor larger than one. So, choosing a normal distribution will result in ascribing positive probability to unsupported values of the parameter. In many cases, this will still work in practice, but will be inefficient for calculating the posterior and will not accurately represent the prior information about the parameter.\n",
    "\n",
    "A common choice in this context is the **beta distribution**, a continuous distribution with 2 parameters and whose support is on the unit interval:\n",
    "\n",
    "$$ f(x \\mid \\alpha, \\beta) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}$$\n",
    "\n",
    "- Support: $x \\in (0, 1)$\n",
    "- Mean: $\\dfrac{\\alpha}{\\alpha + \\beta}$\n",
    "- Variance: $\\dfrac{\\alpha \\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Beta\n",
    "\n",
    "params = (5, 1), (1, 3), (5, 5), (0.5, 0.5), (1, 1)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(params), figsize=(14, 4), sharey=True)\n",
    "for ax, (alpha, beta) in zip(axes, params):\n",
    "    sns.distplot(Beta.dist(alpha, beta).random(size=10000), ax=ax, kde=False)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_title(r'$\\alpha={0}, \\beta={1}$'.format(alpha, beta));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's use a beta distribution to model our prior knowledge of the probabilities for both groups. Setting $\\alpha = \\beta = 1$ will result in a uniform distribution of prior mass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with Model() as ivh_model:\n",
    "    \n",
    "    p = Beta('p', 1, 1, shape=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `p` as the parameter of our Bernoulli likelihood. Here, `x` is a vector of zeros an ones, which will extract the approproate group probability for each subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3 import Bernoulli\n",
    "\n",
    "with ivh_model:\n",
    "    \n",
    "    bb_like = Bernoulli('bb_like', p=p[x], observed=ivh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since we are interested in the difference between the probabilities, we will keep track of this difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with ivh_model:\n",
    "    \n",
    "    p_diff = Deterministic('p_diff', p[1] - p[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ivh_model:\n",
    "    ivh_trace = fit(random_seed=RANDOM_SEED).sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(ivh_trace[100:], varnames=['p'], color='#87ceeb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the probability that `p` is larger for the pneumothorax with probability one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_posterior(ivh_trace[100:], varnames=['p_diff'], ref_val=0, color='#87ceeb');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Resources\n",
    "\n",
    "- Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis, Third Edition. CRC Press.\n",
    "- Kruschke, J.K. *Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan.* 2015. Academic Press / Elsevier. \n",
    "- O'Shea M, Savitz D.A., Hage M.L., Feinstein K.A.: *Prenatal events and the risk of subependymal / intraventricular haemorrhage in very low birth weight neonates*. **Paediatric and Perinatal Epdiemiology** 1992;6:352-362"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
