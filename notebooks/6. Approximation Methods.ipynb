{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximation Methods\n",
    "\n",
    "> “An approximate answer to the right problem is worth a good deal more than\n",
    "an exact answer to an approximate problem.” -- John Tukey\n",
    "\n",
    "Markov chain Monte Carlo (MCMC) is the *de facto* standard for the estimation of Bayesian models. It is an important and useful approach because it is asymptotically exact and can be implemented readily in software and applied to a wide range of probabilistic models. The main drawback of MCMC, however, is its **computational expense**, as it requires repeated calculation of likelihoods and other quantities at every iteration of the algorithm. These calculations typically involve all of the data specified in the model, and hence do not scale well with the size of the dataset being used to fit the model.\n",
    "\n",
    "An alternative to this employs one of several *approximation* methods. By an approximation, we are here referring to methods that do not exactly calculate or sample from the full posterior distribution specified by the model, but rather, either returns one or more moments of the posterior or use an alternative functional form in place of the true posterior distribution. \n",
    "\n",
    "We will outline two of these methods that are available in PyMC3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum a posteriori (MAP) estimation\n",
    "\n",
    "The most straightforward way for obtaining estimates from a Bayesian model is to find the maximum *a posteriori* estimate of the model parameters. This simply involves applying a numerical optimization algorithm to the model, several of which are available in the SciPy package for Python. Since the marginal likelihood is a constant with respect to the parameters, the estimates of the parameters derived from a non-normalized model will be the same as those from a normalized model. \n",
    "\n",
    "$$\\hat{\\theta}_{MAP}(y) = \\text{argmax}_{\\theta} \\frac{Pr(y|\\theta)Pr(\\theta)}{\\int Pr(y|\\theta)Pr(\\theta) d\\theta} = \\text{argmax}_{\\theta} Pr(y|\\theta)Pr(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MAP to obtain estimates for the survival model that we introduced previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load ../data/melanoma_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal, Model, DensityDist, sample\n",
    "from pymc3.math import log, exp\n",
    "\n",
    "with Model() as melanoma_survival:\n",
    "\n",
    "    # Convert censoring indicators to indicators for failure event\n",
    "    failure = (censored==0).astype(int)\n",
    "\n",
    "    # Parameters (intercept and treatment effect) for survival rate\n",
    "    beta = Normal('beta', mu=0.0, sd=1e5, shape=2)\n",
    "\n",
    "    # Survival rates, as a function of treatment\n",
    "    lam = exp(beta[0] + beta[1]*treat)\n",
    "    \n",
    "    # Survival likelihood, accounting for censoring\n",
    "    def logp(failure, value):\n",
    "        return (failure * log(lam) - lam * value).sum()\n",
    "\n",
    "    x = DensityDist('x', logp, observed={'failure':failure, 'value':t})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAP estimate can be obtained in PyMC3 via the `find_MAP` function. As with `sample`, we run `find_MAP` inside a model context, or pass the model explicitly to the function as the `model` parameter.\n",
    "\n",
    "Starting values can be optionally passed as a `dict` to the `start` parameter. By default, `fmin_MAP` uses SciPy's `fmin_bfgs` function to find the maximum, which is an implementation of the [Broyden–Fletcher–Goldfarb–Shanno algorithm](https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm). If there are discrete variables in the model, then `fmin_powell` is used, which is SciPy's implementation of [Powell's method](https://en.wikipedia.org/wiki/Powell%27s_method), a more general algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import find_MAP\n",
    "\n",
    "with melanoma_survival:\n",
    "    estimates = find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model, the MAP estimates are comparable to those we would have obtained using MCMC sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import sample\n",
    "\n",
    "with melanoma_survival:\n",
    "    trace = sample(1000, init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import summary\n",
    "\n",
    "summary(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`find_MAP` only returns estimates unobserved random variables from the model, and does not include deterministic values. If we wish to evaluate a determinsitic quantity, we can construct a Theano function and pass in the relevant parameter values as arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major limitation to using MAP for inference is that there is no associated measure of uncertainty. Hence, `find_MAP` cannot be used for inference. It is useful, however, for getting a sense of typical values the model may take for a particular dataset, and for PyMC3 it is intended to be used to get reasonable starting values for use in MCMC algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "An alternative approach to approximating the posterior disstribution that is difficult to calculate analytically is to perform inference on an **appoximation to the true posterior distribution**. \n",
    "\n",
    "The idea is to choose a convenient approximating density $q(\\theta, \\phi)$, with vector of corresponding parameters $\\phi$. The goal is to select $\\phi$ such that $q(\\theta, \\phi)$ is as similar as possible to the true posterior. We therefore require a loss function that measures the similarity of $q(\\theta, \\phi)$ to $p(\\theta| y)$.\n",
    "\n",
    "The loss function employed by variational inference is the **Kullback-Leibler distance**:\n",
    "\n",
    "$$\\text{KL}[q(\\theta, \\phi) || p(\\theta| y)] = \\int q(\\theta, \\phi) \\frac{q(\\theta, \\phi)}{p(\\theta| y)} d\\theta$$\n",
    "\n",
    "However, this integral is difficult to work with, so instead a proxy to KL, called the evidence lower bound, is minimized instead:\n",
    "\n",
    "$$ELBO = \\mathbb{E}_{q(\\theta)} [\\log p(y, \\theta)] − \\mathbb{E}_{q(θ)} [\\log q(\\theta, \\phi)]$$\n",
    "\n",
    "The first term of the ELBO expression $\\mathbb{E}_{q(\\theta)} [\\log p(y, \\theta)]$ is the expectation of the log joint density under the approximation, while the second term $\\mathbb{E}_{q(θ)} [\\log q(\\theta, \\phi)]$ is called the *entropy* of the variational approximation.\n",
    "\n",
    "Algorithms for performing variational inference are difficult to construct, and this has limited its adoption for applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Differentiation Variational Inference\n",
    "\n",
    "Kucukelbir *et al.* (2015) devised a method for automating the variational inference approach, by making a **flexible choice** for the approximating distribution, and transforming the latent variables to an **unconstrained coordinate space** before fitting the model.\n",
    "\n",
    "ADVI proceeds in three steps:\n",
    "\n",
    "1. Transform the model's latent variables to the real coordinate space\n",
    "2. Specify a normal variational distribution.\n",
    "3. Maximize the variational objective via automatic differentiation and stochastic optimization\n",
    "\n",
    "The ADVI procedure works for differentiable probability models (*i.e.* those comprised of continuous latent variables) only. This is because it requires the calculation of the gradient of the log-joint with respect to the stochastic variables:\n",
    "\n",
    "$$\\nabla_{\\theta} \\log p(y, \\theta)$$\n",
    "\n",
    "The key to making ADVI work is the transformation of constrained parameters to the unconstrained, real coordinate space. This allows us to use a Gaussian distribution as the variational density. As with the classical variational inference algorithm, we impose the **mean field assumption**, whereby the Gaussian distributions over all the parameters can be fully factorized:\n",
    "\n",
    "$$q(\\zeta, \\phi) = \\prod_{j=1}^J N(\\zeta | \\mu_j, \\sigma_j^2)$$\n",
    "\n",
    "where $\\zeta$ are the parameters after tranformation by $T: \\theta \\rightarrow \\zeta$.\n",
    "\n",
    "The inverse of the transform used (*e.g.* log for positive variables, logit for probabilities) and the associated Jacobian allows for a **non-normal variational approximation** on the support of the original variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model() as model: \n",
    "    μ = pm.Normal('μ', mu=0, sd=1, testval=0)\n",
    "    σ = pm.HalfNormal('σ', sd=1)\n",
    "    n = pm.Normal('n', mu=μ, sd=σ, observed=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    \n",
    "    approx = pm.fit(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = pm.sample(1000, init=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "ax = sns.distplot(trace['μ'], label='NUTS')\n",
    "sns.distplot(approx_trace['μ'], label='ADVI')\n",
    "ax.set_title('μ')\n",
    "ax.legend(loc=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Inference of Gaussian mixture model with mini-batch ADVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more realistic application of ADVI, to the estimation of a **Gaussian mixture model**. \n",
    "\n",
    "We can generate some artificial data from a mixuture of two Gaussian components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3 import Normal, Metropolis, sample, MvNormal, Dirichlet, Model, DensityDist, find_MAP, NUTS, Slice\n",
    "import theano.tensor as tt\n",
    "from theano.tensor.nlinalg import det\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_samples = 100\n",
    "rng = np.random.RandomState(42)\n",
    "ms = np.array([[-1, -1.5], [1, 1]])\n",
    "ps = np.array([0.2, 0.8])\n",
    "\n",
    "zs = np.array([rng.multinomial(1, ps) for _ in range(n_samples)]).T\n",
    "xs = [z[:, np.newaxis] * rng.multivariate_normal(m, np.eye(2), size=n_samples)\n",
    "      for z, m in zip(zs, ms)]\n",
    "data = np.sum(np.dstack(xs), axis=2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(data[:, 0], data[:, 1], c='g', alpha=0.5)\n",
    "plt.scatter(ms[0, 0], ms[0, 1], c='r', s=100)\n",
    "plt.scatter(ms[1, 0], ms[1, 1], c='b', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian mixture models are usually constructed with **categorical random variables**. However, ADVI cannot fit models with discrete variables, since it uses the gradient of the model with respect to the parameters. \n",
    "\n",
    "To get around this, we can integrate (analytically) over the latent group indicators. This results in a continuous probability distribution that is the weighted sum of the Gaussian components. The log likelihood of the total probability is calculated using `logsumexp` (LSE), which is a [standard technique for making this kind of calculation stable](https://arxiv.org/abs/1506.03431):\n",
    "\n",
    "$$\\text{logSumExp}(x) = \\log \\left[ \\sum_{i=1}^N x_i - \\max(x) \\right] + \\max(x)$$\n",
    "\n",
    "In the below code, DensityDist class is used as the likelihood term. The second argument, `logp_gmix(mus, pi, np.eye(2))`, is a Python function which recieves observations (denoted by `value`) and returns the tensor representation of the log-likelihood. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc3.math import logsumexp\n",
    "\n",
    "# Log likelihood of normal distribution\n",
    "def logp_normal(mu, tau, value):\n",
    "    # log probability of individual samples\n",
    "    k = tau.shape[0]\n",
    "    delta = lambda mu: value - mu\n",
    "    return (-1 / 2.) * (k * tt.log(2 * np.pi) + tt.log(1./det(tau)) +\n",
    "                         (delta(mu).dot(tau) * delta(mu)).sum(axis=1))\n",
    "\n",
    "# Log likelihood of Gaussian mixture distribution\n",
    "def logp_gmix(mus, pi, tau):\n",
    "    def logp_(value):        \n",
    "        logps = [tt.log(pi[i]) + logp_normal(mu, tau, value)\n",
    "                 for i, mu in enumerate(mus)]\n",
    "            \n",
    "        return tt.sum(logsumexp(tt.stacklists(logps)[:, :n_samples], axis=0))\n",
    "\n",
    "    return logp_\n",
    "\n",
    "with pm.Model() as model:\n",
    "    μ = [MvNormal('μ_%d' % i, mu=np.zeros(2), tau=0.1 * np.eye(2), shape=(2,))\n",
    "           for i in range(2)]\n",
    "    π = Dirichlet('π', a=0.1 * np.ones(2), shape=(2,))\n",
    "    x = DensityDist('x', logp_gmix(μ, π, np.eye(2)), observed=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison with ADVI, run MCMC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    trace = sample(1000, tune=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check posterior of component means and weights. We can see that the MCMC samples of the component means differed in variance due to the difference of the sample size of these clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "mu_0, mu_1 = trace['μ_0'], trace['μ_1']\n",
    "plt.scatter(mu_0[:, 0], mu_0[:, 1], alpha=0.1)\n",
    "plt.scatter(mu_1[:, 0], mu_1[:, 1], alpha=0.1)\n",
    "plt.scatter(data[:, 0], data[:, 1], c='k', alpha=0.3)\n",
    "\n",
    "plt.scatter(ms[0, 0], ms[0, 1], c='r', s=100)\n",
    "plt.scatter(ms[1, 0], ms[1, 1], c='b', s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot([1, 2], np.mean(trace['π'], axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit the same model with ADVI as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "\n",
    "    approx = pm.fit(n=20000, obj_optimizer=pm.adam(learning_rate=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns three variables. `means` and `sds` are the mean and standard deviations of the variational posterior (*Note that these values are in the transformed space, not in the original space*). \n",
    "\n",
    "But, we can see the variational posterior in the original space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_trace = approx.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "mu_0, mu_1 = approx_trace['μ_0'], approx_trace['μ_1']\n",
    "plt.scatter(mu_0[:, 0], mu_0[:, 1], alpha=0.1)\n",
    "plt.scatter(mu_1[:, 0], mu_1[:, 1], alpha=0.1)\n",
    "plt.scatter(data[:, 0], data[:, 1], c='k', alpha=0.3)\n",
    "\n",
    "plt.scatter(ms[0, 0], ms[0, 1], c='r', s=100)\n",
    "plt.scatter(ms[1, 0], ms[1, 1], c='b', s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`elbos` contains the trace of the evidence lower bound, showing stochastic convergence of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(approx.hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate that ADVI works for large dataset with mini-batch, let's create 100,000 samples from the same mixture distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000\n",
    "\n",
    "zs = np.array([rng.multinomial(1, ps) for _ in range(n_samples)]).T\n",
    "xs = [z[:, np.newaxis] * rng.multivariate_normal(m, np.eye(2), size=n_samples)\n",
    "      for z, m in zip(zs, ms)]\n",
    "data = np.sum(np.dstack(xs), axis=2)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(data[:, 0], data[:, 1], c='k', alpha=0.01)\n",
    "plt.scatter(ms[0, 0], ms[0, 1], s=100)\n",
    "plt.scatter(ms[1, 0], ms[1, 1], s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    μ = [MvNormal('μ_%d' % i, mu=np.zeros(2), tau=0.1 * np.eye(2), shape=(2,))\n",
    "           for i in range(2)]\n",
    "    π = Dirichlet('π', a=0.1 * np.ones(2), shape=(2,))\n",
    "    x = DensityDist('x', logp_gmix(μ, π, np.eye(2)), observed=data)\n",
    "    \n",
    "    trace = sample(10000, step=Metropolis(), init=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posterior samples are concentrated on the true means, so looks like single point for each component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.1, c='k')\n",
    "mu_0, mu_1 = trace['μ_0'], trace['μ_1']\n",
    "plt.scatter(mu_0[-500:, 0], mu_0[-500:, 1], alpha=0.4)\n",
    "plt.scatter(mu_1[-500:, 0], mu_1[-500:, 1], alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ADVI with mini-batch, pass a Theano `tensor` to the likelihood (an `ObservedRV`). The tensor will iteratively be replaced with mini-batches during the ADVI run. Because of the difference of the size of mini-batch and whole samples, the log-likelihood term needs to be appropriately scaled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pm.Minibatch(data, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    μ = [MvNormal('μ_%d' % i, mu=np.zeros(2), tau=0.1 * np.eye(2), shape=(2,))\n",
    "           for i in range(2)]\n",
    "    π = Dirichlet('π', a=0.1 * np.ones(2), shape=(2,))\n",
    "    x = DensityDist('x', logp_gmix(μ, π, np.eye(2)), observed=X, total_size=data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADVI model fitting is much faster than MCMC, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    minibatch_fit = pm.fit(n=20000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minibatch_trace = minibatch_fit.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(data[:, 0], data[:, 1], c='k', alpha=0.01)\n",
    "\n",
    "mu_0, mu_1 = minibatch_trace['μ_0'], minibatch_trace['μ_1']\n",
    "plt.scatter(mu_0[:, 0], mu_0[:, 1], alpha=0.05)\n",
    "plt.scatter(mu_1[:, 0], mu_1[:, 1], alpha=0.05)\n",
    "\n",
    "plt.scatter(ms[0, 0], ms[0, 1], s=10)\n",
    "plt.scatter(ms[1, 0], ms[1, 1], s=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but the result is almost the same (at least for this simple model). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "mu_0, sd_0 = means['μ_0'], sds['μ_0']\n",
    "mu_1, sd_1 = means['μ_1'], sds['μ_1']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(data[:, 0], data[:, 1], alpha=0.2, c='k')\n",
    "plt.scatter(mu_0[0], mu_0[1])\n",
    "plt.scatter(mu_1[0], mu_1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance of the trace of ELBO is larger than without mini-batch because of the subsampling from the whole samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(minibatch_fit.hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1.\t[Kucukelbir A, Ranganath R, Gelman A, Blei DM.](https://arxiv.org/abs/1506.03431) Automatic Variational Inference in Stan. arXiv. 2015;stat.ML."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
